#### 사전 준비
- request 는 내장 모듈
- BeutifulSoup4 설치
```
pip install BeutifulSoup4
```

```python
>>> import os, re
>>> os.chdir(r'c:\users\ramy\desktop\test')
>>> import usecsv
>>> import urllib.request as ur
>>> from bs4 import BeautifulSoup as bs
```

#### quotes to scrape 사이트 분석
```python
>>> url = "http://quotes.toscrape.com/"
>>> ur.urlopen(url)
<http.client.HTTPResponse object at 0x0000025F557F6DD0>

>>> html = ur.urlopen(url)
>>> html.read()[:100]
b'<!DOCTYPE html>\n<html lang="en">\n<head>\n\t<meta charset="UTF-8">\n\t<title>Quotes to Scrape</title>\n   '
```

##### 뷰티풀 수프로 자료형 변환하기
```python
bs(html.read(), 'html.parser')
```

```python
>>> html = ur.urlopen(url)
>>> soup = bs(html.read(), 'html.parser')

>>> type(html)
<class 'http.client.HTTPResponse'>
>>> type(soup)
<class 'bs4.BeautifulSoup'>
```
- 👉 soup의 자료형이 bs4.BeautifulSoup로 이뤄저 있다.

```ad-tip
- BeutifulSoup 핵심코드
	```python
	soup = bs(ur.urlopen(URL주소).read(), 'html.parser')
	
	soup = bs(ur.urlopen("http://quotes.toscrape.com/").read(), 'html.parser')
	```
```


#### 원하는 태그 모으기(find_all)
```python
soup.find_all(찾아낼_태그)

>>> soup.find_all('div', {"class":"quote"})

>>> soup.find_all('div', {"class":"quote"})[0].text

>>> print(soup.find_all('div', {"class":"quote"})[0].text)

for i in soup.find_all('div', {"class": "quote"}):
	print(i.text)
```

#### 크롤링으로 a태그 href 속성 가져오기
```python
a.get('속성')

>>> for i in soup.find_all('a')[:5]:
		i.get('href')

>>> for i in soup.find_all('a', {"class":"link_txt"}):
...     i.find_all('a')[0].get('href')

```
- 사이트 구조가 변경될 수도 있기 때문에, 홈페이지의 태그들을 보면서 필요한 태그 구조를 잘 파악해야 한다. 

```ad-tip
- find_all 은 필요한 태그를 추출하는 역할,
- get 은 태그 안에 있는 속성의 값을 추출할 때 사용
```

```python
>>> article1 = 'https://v.daum.net/v/20230617150257746'

>>> soup = bs(ur.urlopen(article1).read(), 'html.parser')

>>> for i in soup.find_all('p'):
...     print(i.text)
```


- 👉 이렇게 헤드라인 > 기사내용 > 순으로 일자별 신문기사 스크랩도 가능하다.

#### 웹 크롤링 실행 파일 만들기
- pip로 pyinstaller 설치하기
```shell
pip install pyinstaller
```
`
```python
# 작성한 파이썬 파일
# scrap_article.py

import time
import re, os
import urllib.request as ur
from bs4 import BeautifulSoup as bs
url = "https://news.daum.net/"

soup = bs(ur.urlopen(url).read(), 'html.parser')

os.chdir(r'c:\users\ramy\desktop\test')

today = time.strftime('%m@%d', time.localtime(time.time()))
f = open('article_{0}.txt'.format(today), 'w')

for i in soup.find_all('a', {"class":"link_txt"}):
	if i.text == "뉴스홈":
		break
	try:
		f.write(i.text + '\n')
		temp_article_url = i.get('href')
		f.write(temp_article_url + '\n')
		soup2 = bs(ur.urlopen(temp_article_url).read(), 'html.parser')
		
		for j in soup2.find_all('p'):
			if "이 글자크기로" in j.text:
				continue
			if "(예시)" in j.text:
				continue
			if "톡방" in j.text:
				continue
			if "무단 전재" in j.text:
				continue
			f.write(j.text +'\n')
	except:
		pass

f.close()
```

- 실행 파일 만들기
	- 명령 프롬프트에서 실행 파일로 만들고 싶은 파일이 저장된 경로로 이동 후 명령어 입력
```shell
pyinstaller --onefile [파이썬 파일].py

$ pyinstaller --onefile article_collector.py
```

- dist, build 디렉터리 각각 생성 후, dist 내의 실행 파일 실행시키면 해당 프로그램 실행
![](assets/06.%20웹%20크롤링.png)