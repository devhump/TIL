#### ì‚¬ì „ ì¤€ë¹„
- request ëŠ” ë‚´ì¥ ëª¨ë“ˆ
- BeutifulSoup4 ì„¤ì¹˜
```
pip install BeutifulSoup4
```

```python
>>> import os, re
>>> os.chdir(r'c:\users\ramy\desktop\test')
>>> import usecsv
>>> import urllib.request as ur
>>> from bs4 import BeautifulSoup as bs
```

#### quotes to scrape ì‚¬ì´íŠ¸ ë¶„ì„
```python
>>> url = "http://quotes.toscrape.com/"
>>> ur.urlopen(url)
<http.client.HTTPResponse object at 0x0000025F557F6DD0>

>>> html = ur.urlopen(url)
>>> html.read()[:100]
b'<!DOCTYPE html>\n<html lang="en">\n<head>\n\t<meta charset="UTF-8">\n\t<title>Quotes to Scrape</title>\n   '
```

##### ë·°í‹°í’€ ìˆ˜í”„ë¡œ ìë£Œí˜• ë³€í™˜í•˜ê¸°
```python
bs(html.read(), 'html.parser')
```

```python
>>> html = ur.urlopen(url)
>>> soup = bs(html.read(), 'html.parser')

>>> type(html)
<class 'http.client.HTTPResponse'>
>>> type(soup)
<class 'bs4.BeautifulSoup'>
```
- ğŸ‘‰ soupì˜ ìë£Œí˜•ì´ bs4.BeautifulSoupë¡œ ì´ë¤„ì € ìˆë‹¤.

```ad-tip
- BeutifulSoup í•µì‹¬ì½”ë“œ
	```python
	soup = bs(ur.urlopen(URLì£¼ì†Œ).read(), 'html.parser')
	
	soup = bs(ur.urlopen("http://quotes.toscrape.com/").read(), 'html.parser')
	```
```


#### ì›í•˜ëŠ” íƒœê·¸ ëª¨ìœ¼ê¸°(find_all)
```python
soup.find_all(ì°¾ì•„ë‚¼_íƒœê·¸)

>>> soup.find_all('div', {"class":"quote"})

>>> soup.find_all('div', {"class":"quote"})[0].text

>>> print(soup.find_all('div', {"class":"quote"})[0].text)

for i in soup.find_all('div', {"class": "quote"}):
	print(i.text)
```

#### í¬ë¡¤ë§ìœ¼ë¡œ aíƒœê·¸ href ì†ì„± ê°€ì ¸ì˜¤ê¸°
```python
a.get('ì†ì„±')

>>> for i in soup.find_all('a')[:5]:
		i.get('href')

>>> for i in soup.find_all('a', {"class":"link_txt"}):
...     i.find_all('a')[0].get('href')

```
- ì‚¬ì´íŠ¸ êµ¬ì¡°ê°€ ë³€ê²½ë  ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì—, í™ˆí˜ì´ì§€ì˜ íƒœê·¸ë“¤ì„ ë³´ë©´ì„œ í•„ìš”í•œ íƒœê·¸ êµ¬ì¡°ë¥¼ ì˜ íŒŒì•…í•´ì•¼ í•œë‹¤. 

```ad-tip
- find_all ì€ í•„ìš”í•œ íƒœê·¸ë¥¼ ì¶”ì¶œí•˜ëŠ” ì—­í• ,
- get ì€ íƒœê·¸ ì•ˆì— ìˆëŠ” ì†ì„±ì˜ ê°’ì„ ì¶”ì¶œí•  ë•Œ ì‚¬ìš©
```

```python
>>> article1 = 'https://v.daum.net/v/20230617150257746'

>>> soup = bs(ur.urlopen(article1).read(), 'html.parser')

>>> for i in soup.find_all('p'):
...     print(i.text)
```


- ğŸ‘‰ ì´ë ‡ê²Œ í—¤ë“œë¼ì¸ > ê¸°ì‚¬ë‚´ìš© > ìˆœìœ¼ë¡œ ì¼ìë³„ ì‹ ë¬¸ê¸°ì‚¬ ìŠ¤í¬ë©ë„ ê°€ëŠ¥í•˜ë‹¤.

#### ì›¹ í¬ë¡¤ë§ ì‹¤í–‰ íŒŒì¼ ë§Œë“¤ê¸°
- pipë¡œ pyinstaller ì„¤ì¹˜í•˜ê¸°
```shell
pip install pyinstaller
```
`
```python
# ì‘ì„±í•œ íŒŒì´ì¬ íŒŒì¼
# scrap_article.py

import time
import re, os
import urllib.request as ur
from bs4 import BeautifulSoup as bs
url = "https://news.daum.net/"

soup = bs(ur.urlopen(url).read(), 'html.parser')

os.chdir(r'c:\users\ramy\desktop\test')

today = time.strftime('%m@%d', time.localtime(time.time()))
f = open('article_{0}.txt'.format(today), 'w')

for i in soup.find_all('a', {"class":"link_txt"}):
	if i.text == "ë‰´ìŠ¤í™ˆ":
		break
	try:
		f.write(i.text + '\n')
		temp_article_url = i.get('href')
		f.write(temp_article_url + '\n')
		soup2 = bs(ur.urlopen(temp_article_url).read(), 'html.parser')
		
		for j in soup2.find_all('p'):
			if "ì´ ê¸€ìí¬ê¸°ë¡œ" in j.text:
				continue
			if "(ì˜ˆì‹œ)" in j.text:
				continue
			if "í†¡ë°©" in j.text:
				continue
			if "ë¬´ë‹¨ ì „ì¬" in j.text:
				continue
			f.write(j.text +'\n')
	except:
		pass

f.close()
```

- ì‹¤í–‰ íŒŒì¼ ë§Œë“¤ê¸°
	- ëª…ë ¹ í”„ë¡¬í”„íŠ¸ì—ì„œ ì‹¤í–‰ íŒŒì¼ë¡œ ë§Œë“¤ê³  ì‹¶ì€ íŒŒì¼ì´ ì €ì¥ëœ ê²½ë¡œë¡œ ì´ë™ í›„ ëª…ë ¹ì–´ ì…ë ¥
```shell
pyinstaller --onefile [íŒŒì´ì¬ íŒŒì¼].py

$ pyinstaller --onefile article_collector.py
```

- dist, build ë””ë ‰í„°ë¦¬ ê°ê° ìƒì„± í›„, dist ë‚´ì˜ ì‹¤í–‰ íŒŒì¼ ì‹¤í–‰ì‹œí‚¤ë©´ í•´ë‹¹ í”„ë¡œê·¸ë¨ ì‹¤í–‰
![](assets/06.%20ì›¹%20í¬ë¡¤ë§.png)